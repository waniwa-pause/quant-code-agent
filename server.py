import os
import requests  # æ–°å¢ï¼šç”¨äºè°ƒç”¨ Backtrader API
from contextlib import asynccontextmanager
from typing import Annotated, Literal
from typing_extensions import TypedDict

from fastapi import FastAPI
from pydantic import BaseModel

# LangChain æ ¸å¿ƒ
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage
from langchain_core.documents import Document
from langchain_core.tools import tool # æ–°å¢ï¼šå·¥å…·å®šä¹‰

# LangGraph å›¾æ„å»º
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.prebuilt import ToolNode # æ–°å¢ï¼šå·¥å…·èŠ‚ç‚¹

# æ•°æ®åº“ä¸å‘é‡
from psycopg_pool import ConnectionPool
from langchain_postgres import PGVector
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. æ•°æ®åº“ä¸å‘é‡åº“åˆå§‹åŒ– ---
DB_URI = os.getenv("DB_URI")
connection_pool = ConnectionPool(conninfo=DB_URI, min_size=1, max_size=10, kwargs={"autocommit": True})

print("ğŸ”„ åˆå§‹åŒ– Embedding æ¨¡å‹...")
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

vector_store = PGVector(
    embeddings=embeddings,
    collection_name="knowledge_base",
    connection=DB_URI,
    use_jsonb=True,
)
print("âœ… å‘é‡æ•°æ®åº“å°±ç»ª")

# --- 2. å®šä¹‰å›æµ‹å·¥å…· (Backtrader Tool) ---
@tool
def execute_backtest(strategy_code: str, start_cash: float = 100000.0):
    """
    æ‰§è¡Œé‡åŒ–ç­–ç•¥å›æµ‹ã€‚
    Args:
        strategy_code: Python ä»£ç å­—ç¬¦ä¸²ã€‚
                       è¦æ±‚ï¼šå¿…é¡»åŒ…å«ä¸€ä¸ªåä¸º 'GeneratedStrategy' çš„ç±»ï¼Œç»§æ‰¿è‡ª bt.Strategyã€‚
                       é€»è¾‘å¿…é¡»å†™åœ¨ next(self) æ–¹æ³•ä¸­ã€‚
        start_cash: åˆå§‹èµ„é‡‘ï¼Œé»˜è®¤ 100000ã€‚
    """
    # Docker å†…éƒ¨ç½‘ç»œï¼šç›´æ¥è®¿é—®æœåŠ¡å 'backtrader_engine'
    url = "http://backtrader_engine:8001/run_backtest"
    payload = {
        "code": strategy_code,
        "start_cash": start_cash
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        if response.status_code == 200:
            return response.json() # è¿”å›å›æµ‹ç»“æœï¼ˆç›ˆäºã€æ—¥å¿—ï¼‰
        else:
            return f"å›æµ‹æœåŠ¡æŠ¥é”™: {response.text}"
    except Exception as e:
        return f"æ— æ³•è¿æ¥åˆ°å›æµ‹å¼•æ“: {str(e)}"

# --- 3. å®šä¹‰ Agent æ¨¡å‹ä¸çŠ¶æ€ ---
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]

api_key = os.getenv("DEEPSEEK_API_KEY")
model = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=api_key,
    base_url="https://api.deepseek.com",
    temperature=0.7
)

# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ (è®© Agent çŸ¥é“å®ƒèƒ½åšä»€ä¹ˆ)
tools = [execute_backtest]
model_with_tools = model.bind_tools(tools)

# --- 4. æ ¸å¿ƒé€»è¾‘èŠ‚ç‚¹ ---
def agent_node(state: AgentState):
    messages = state["messages"]
    last_user_msg = messages[-1]
    
    # A. RAG æ£€ç´¢ (ä»…å¯¹ç”¨æˆ·æ¶ˆæ¯è¿›è¡Œæ£€ç´¢)
    if isinstance(last_user_msg, HumanMessage):
        query = last_user_msg.content
        try:
            # æ£€ç´¢ç›¸å…³çš„ 1 æ¡çŸ¥è¯†
            docs = vector_store.similarity_search(query, k=1)
            if docs:
                context = docs[0].page_content
                print(f"ğŸ“š RAG å‘½ä¸­: {context[:20]}...")
                # å°†çŸ¥è¯†ä½œä¸º SystemMessage æ’å…¥åˆ°å†å²æ¶ˆæ¯å‰ï¼Œæˆ–è€…æ‹¼æ¥åˆ°æœ€åä¸€æ¡
                # è¿™é‡Œç®€å•å¤„ç†ï¼šæ‹¼æ¥åˆ° Prompt
                query = f"ã€å‚è€ƒèƒŒæ™¯çŸ¥è¯†ã€‘ï¼š{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{query}"
                # æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹ï¼ˆä¸æ”¹å˜ç±»å‹ï¼Œä»…å¢å¼ºä¸Šä¸‹æ–‡ï¼‰
                messages[-1] = HumanMessage(content=query)
        except Exception as e:
            print(f"âš ï¸ RAG æ£€ç´¢è·³è¿‡: {e}")

    # B. è°ƒç”¨æ¨¡å‹
    response = model_with_tools.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["tools", END]:
    last_message = state["messages"][-1]
    # å¦‚æœæ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·ï¼Œè·³è½¬åˆ° tools èŠ‚ç‚¹
    if last_message.tool_calls:
        return "tools"
    return END

# --- 5. æ„å»ºå›¾ (Workflow) ---
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools)) # ä¸“é—¨æ‰§è¡Œå·¥å…·çš„èŠ‚ç‚¹

# å®šä¹‰è¾¹
workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", should_continue)
workflow.add_edge("tools", "agent") # å·¥å…·æ‰§è¡Œå®Œï¼Œç»“æœå›ä¼ ç»™ Agent ç»§ç»­æ€è€ƒ

# --- 6. FastAPI æœåŠ¡ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ Agent å…¨åŠŸèƒ½æœåŠ¡å¯åŠ¨ (RAG + Backtrader)...")
    yield
    print("ğŸ›‘ æœåŠ¡å…³é—­...")
    connection_pool.close()

app = FastAPI(lifespan=lifespan)

class ChatRequest(BaseModel):
    message: str
    thread_id: str

@app.post("/chat")
def chat_endpoint(request: ChatRequest):
    with connection_pool.connection() as conn:
        checkpointer = PostgresSaver(conn)
        checkpointer.setup()
        app_graph = workflow.compile(checkpointer=checkpointer)
        
        config = {"configurable": {"thread_id": request.thread_id}}
        # è¿™é‡Œçš„ messages å¿…é¡»æ˜¯åˆ—è¡¨
        final_state = None
        for event in app_graph.stream(
            {"messages": [HumanMessage(content=request.message)]}, 
            config=config
        ):
            final_state = event
            
        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
        last_msg = final_state[list(final_state.keys())[0]]["messages"][-1]
        return {"response": last_msg.content}

# çŸ¥è¯†å…¥åº“æ¥å£ (ä¿æŒä¸å˜)
class IngestRequest(BaseModel):
    text: str

@app.post("/ingest")
def ingest_endpoint(request: IngestRequest):
    doc = Document(page_content=request.text, metadata={"source": "api"})
    try:
        vector_store.add_documents([doc])
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "message": str(e)}
