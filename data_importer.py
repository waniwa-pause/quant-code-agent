import os
import io
import time
import shutil
import rarfile
import pandas as pd
import re  # âœ… æ–°å¢ï¼šç”¨äºæ­£åˆ™æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸ
from sqlalchemy import create_engine
from urllib.parse import quote_plus

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
DB_CONFIG = {
    "user": "user",
    "password": "password",
    "host": "localhost",
    # æ³¨æ„ï¼šDocker æ˜ å°„å‡ºæ¥çš„ç«¯å£æ˜¯ 5433
    "port": "5433",
    # æ•°æ®å°†å­˜å…¥ quant_db åº“ (æ‚¨åˆšæ‰æ‰¾åˆ°æ•°æ®çš„åœ°æ–¹)
    "dbname": "quant_db" 
}

# æ•°æ®æºæ ¹ç›®å½•
DATA_ROOT = r'D:\å•†å“æ•°æ®'

# âœ… ä¿®æ”¹ 1: è¡¥å……äº† '2010' ä»¥åŠå…¶ä»–å¹´ä»½ï¼Œè„šæœ¬å°†ä¾æ¬¡å¤„ç†è¿™äº›æ–‡ä»¶å¤¹
TARGET_FOLDERS = ['2010', '2011', '2012', '2013', '2014', '2015', '2016']

# ä¸´æ—¶è§£å‹ç›®å½• (è„šæœ¬è¿è¡Œå®Œä¼šè‡ªåŠ¨æ¸…ç†)
TEMP_FOLDER = './temp_extracted_data'

# æ•°æ®åº“ç›®æ ‡è¡¨å
TARGET_TABLE = 'futures_tick_data'

# âœ… ä¿®æ”¹ 2: å®šä¹‰éœ€è¦çš„â€œæ ‡å‡†åˆ—â€
# è„šæœ¬ä¼šä¸¢å¼ƒ CSV ä¸­ä¸åœ¨æ­¤åˆ—è¡¨é‡Œçš„å…¶ä»–åˆ—ï¼ˆå¦‚å…¨0åˆ—ï¼‰
COLUMN_MAPPING = [
    'å¸‚åœºä»£ç ', 'åˆçº¦ä»£ç ', 'æ—¶é—´', 'æœ€æ–°', 'æŒä»“', 'å¢ä»“', 
    'æˆäº¤é¢', 'æˆäº¤é‡', 'å¼€ä»“', 'å¹³ä»“', 'æˆäº¤ç±»å‹', 'æ–¹å‘', 
    'ä¹°ä¸€ä»·', 'å–ä¸€ä»·', 'ä¹°ä¸€é‡', 'å–ä¸€é‡'
]
# ===========================================

def get_engine():
    """å»ºç«‹æ•°æ®åº“è¿æ¥å¼•æ“"""
    uri = f"postgresql+psycopg2://{DB_CONFIG['user']}:{quote_plus(DB_CONFIG['password'])}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}"
    return create_engine(uri)

def fast_copy_to_db(df, engine):
    """
    ä½¿ç”¨ PostgreSQL çš„ COPY FROM æŒ‡ä»¤è¿›è¡Œæé€Ÿå…¥åº“ã€‚
    æ¯”æ™®é€šçš„ to_sql å¿«å¾ˆå¤šï¼Œé€‚åˆå¤§æ‰¹é‡ Tick æ•°æ®ã€‚
    """
    if df.empty: return True
    
    conn = engine.raw_connection()
    cursor = conn.cursor()
    
    # ç¡®ä¿ DataFrame åˆ—åä¸æ˜ å°„ä¸€è‡´
    if len(df.columns) == len(COLUMN_MAPPING): 
        df.columns = COLUMN_MAPPING
        
    # å°† NaN/None æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ COPY æŠ¥é”™
    df = df.where(pd.notnull(df), None)
    
    # ä½¿ç”¨å†…å­˜ç¼“å†²åŒºæ¨¡æ‹Ÿæ–‡ä»¶å¯¹è±¡
    output = io.StringIO()
    df.to_csv(output, sep='\t', header=False, index=False)
    output.seek(0)
    
    try:
        # æ ¸å¿ƒï¼šç›´æ¥æŠŠå†…å­˜ä¸­çš„ CSV æ•°æ®æµ copy è¿›æ•°æ®åº“
        cursor.copy_from(output, TARGET_TABLE, null='', sep='\t')
        conn.commit()
        return True
    except Exception as e:
        conn.rollback()
        print(f"X({e})", end="", flush=True)
        return False
    finally:
        cursor.close()
        conn.close()

def main():
    engine = get_engine()
    
    # åˆå§‹åŒ–ï¼šæ¸…ç†å¹¶é‡å»ºä¸´æ—¶ç›®å½•
    if os.path.exists(TEMP_FOLDER): shutil.rmtree(TEMP_FOLDER)
    os.makedirs(TEMP_FOLDER)

    # æ‰¹å¤„ç†å¤§å°ï¼šæ¯ç´¯ç§¯ 100 ä¸ª CSV æ–‡ä»¶å…¥åº“ä¸€æ¬¡
    BATCH_SIZE = 100 
    
    print(f"ğŸ¯ å‡†å¤‡å¤„ç†å¹´ä»½: {TARGET_FOLDERS}")

    # --- ç¬¬ä¸€å±‚å¾ªç¯ï¼šéå†å¹´ä»½æ–‡ä»¶å¤¹ ---
    for year_folder in TARGET_FOLDERS:
        current_path = os.path.join(DATA_ROOT, year_folder)
        
        print(f"\n{'='*50}")
        print(f"ğŸ“‚ è¿›å…¥æ–‡ä»¶å¤¹: {current_path}")

        if not os.path.exists(current_path):
            print(f"âš ï¸  [è·³è¿‡] æ‰¾ä¸åˆ°è·¯å¾„: {current_path}")
            continue

        # å¯»æ‰¾ .rar æ–‡ä»¶
        rar_files = [f for f in os.listdir(current_path) if f.lower().endswith('.rar')]
        rar_files.sort()
        
        if not rar_files:
            print(f"âš ï¸  [è·³è¿‡] {year_folder} é‡Œæ²¡æœ‰ .rar æ–‡ä»¶")
            continue

        print(f"âœ… å‘ç° {len(rar_files)} ä¸ªå‹ç¼©åŒ…ï¼Œå¼€å§‹å¤„ç†...")

        # --- ç¬¬äºŒå±‚å¾ªç¯ï¼šå¤„ç†æ¯ä¸ªå‹ç¼©åŒ… ---
        for idx, rar_file in enumerate(rar_files):
            rar_path = os.path.join(current_path, rar_file)
            print(f"[{idx+1}/{len(rar_files)}] {year_folder}/{rar_file} ...", end="", flush=True)
            
            try:
                # ä½¿ç”¨ rarfile åº“è§£å‹
                with rarfile.RarFile(rar_path) as rf:
                    rf.extractall(TEMP_FOLDER)
                    csv_files = [f for f in rf.namelist() if f.lower().endswith('.csv')]
                    
                    print(f" è§£å‹ {len(csv_files)} CSV | æ¸…æ´—å…¥åº“", end="", flush=True)
                    
                    dfs_buffer = [] 
                    
                    # --- ç¬¬ä¸‰å±‚å¾ªç¯ï¼šè¯»å– CSV å¹¶æ¸…æ´— ---
                    for i, csv_f in enumerate(csv_files):
                        full_path = os.path.join(TEMP_FOLDER, csv_f)
                        try:
                            # å°è¯•ä¸åŒç¼–ç è¯»å–
                            try: df = pd.read_csv(full_path, encoding='gbk')
                            except: df = pd.read_csv(full_path, encoding='utf-8')
                            
                            # ==================== âœ… æ ¸å¿ƒä¿®æ”¹ A: æ™ºèƒ½ç­›é€‰åˆ— ====================
                            # ç›®çš„ï¼šå»é™¤å¤šä½™çš„å…¨0åˆ—ï¼Œåªä¿ç•™ COLUMN_MAPPING é‡Œçš„åˆ—
                            if set(COLUMN_MAPPING).issubset(df.columns):
                                # å¦‚æœè¡¨å¤´é½å…¨ï¼Œç›´æ¥æŒ‰åˆ—åæå–ï¼ˆæœ€å®‰å…¨ï¼‰
                                df = df[COLUMN_MAPPING]
                            else:
                                # å¦‚æœè¡¨å¤´å¯¹ä¸ä¸Šï¼Œæˆ–è€…æœ‰å¤šä½™åˆ—ï¼Œå¼ºåˆ¶æˆªå–å‰ N åˆ—
                                df = df.iloc[:, :len(COLUMN_MAPPING)]
                                df.columns = COLUMN_MAPPING
                            
                            # ==================== âœ… æ ¸å¿ƒä¿®æ”¹ B: è¡¥å…¨æ—¥æœŸ ====================
                            # ç›®çš„ï¼šå°† "09:15:00" å˜æˆ "2010-01-04 09:15:00"
                            
                            # 1. ä¼˜å…ˆä» RAR æ–‡ä»¶åæ‰¾8ä½æ•°å­— (å¦‚ 20100104.rar)
                            date_match = re.search(r"(\d{8})", rar_file)
                            
                            # 2. æ‰¾ä¸åˆ°åˆ™å» CSV æ–‡ä»¶åæ‰¾
                            if not date_match:
                                date_match = re.search(r"(\d{8})", csv_f)
                                
                            if date_match:
                                raw_date = date_match.group(1) # æ‹¿åˆ° "20100104"
                                # æ ¼å¼åŒ–ä¸º "2010-01-04"
                                date_str = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:]}"
                                
                                # 3. æ‹¼æ¥ï¼šæ—¥æœŸ + ç©ºæ ¼ + åŸæ—¶é—´åˆ—
                                if 'æ—¶é—´' in df.columns:
                                    # astype(str) é˜²æ­¢æ—¶é—´åˆ—è¢«è¯†åˆ«ä¸ºå¯¹è±¡å¯¼è‡´æŠ¥é”™
                                    df['æ—¶é—´'] = date_str + ' ' + df['æ—¶é—´'].astype(str)
                            # ==============================================================

                            dfs_buffer.append(df)
                            
                            # ç¼“å†²åŒºæ»¡æˆ–æ–‡ä»¶å¤„ç†å®Œæ—¶ï¼Œæ‰§è¡Œå…¥åº“
                            if len(dfs_buffer) >= BATCH_SIZE or i == len(csv_files) - 1:
                                if dfs_buffer:
                                    big_df = pd.concat(dfs_buffer, ignore_index=True)
                                    if fast_copy_to_db(big_df, engine):
                                        print(".", end="", flush=True)
                                    else:
                                        print("X", end="", flush=True)
                                    dfs_buffer = [] # æ¸…ç©ºç¼“å†²
                        except Exception as e:
                            # å•ä¸ªCSVå‡ºé”™ä¸ä¸­æ–­æ•´ä½“
                            pass
                        finally:
                            # è¯»å®Œå³åˆ ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
                            if os.path.exists(full_path): os.remove(full_path)
                
                print(" å®Œæˆ", flush=True)
                
            except Exception as e:
                print(f"\n[é”™è¯¯] å¤„ç†å‹ç¼©åŒ…å¤±è´¥: {e}", flush=True)
        # --- End of RAR loop ---

    print(f"\n{'='*50}")
    print(f"ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼è¯·å»æ•°æ®åº“æ£€æŸ¥æ•°æ®ã€‚")

if __name__ == "__main__":
    main()